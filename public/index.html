<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <link href="assets/style.css" type="text/css" rel="stylesheet">
    <script src="https://distill.pub/template.v1.js"></script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script src="https://d3js.org/d3.v4.min.js"></script>
    <script src="https://d3js.org/d3-selection-multi.v1.min.js"></script>

    <script type="text/front-matter">
      title: "Understanding FiLM"
      description: "Description of the post"
      authors:
      - Vincent Dumoulin: https://vdumoulin.github.io
      - Ethan Perez: http://ethanperez.net/
      - Florian Strub: https://fstrub95.github.io/
      - Harm de Vries: http://www-etud.iro.umontreal.ca/~devries/
      - Aaron Courville: https://aaroncourville.wordpress.com/
      - Yoshua Bengio: http://www.iro.umontreal.ca/~bengioy/yoshua_en/
      affiliations:
      - MILA: https://mila.quebec/en/
      - Rice University: http://www.rice.edu/
      - Inria SequeL: https://team.inria.fr/sequel/
      - MILA: https://mila.quebec/en/
      - MILA: https://mila.quebec/en/
      - MILA: https://mila.quebec/en/
    </script>
  </head>
  <body>
    <dt-article>
      <h1>Understanding feature-wise linear modulation (FiLM) layers</h1>
      <h2><span class="todo"> TODO: Write a description of the article.</span></h2>

      <dt-byline></dt-byline>

      <h2>Introduction</h2>
      <p>
        This article examines the way we condition neural networks, i.e., the
        way in which the computation they carry out is altered by conditioning
        information.
      </p>
      <p>
        The need for conditioning neural networks arises in many contexts:
      </p>
      <ul>
        <li>
          A class-conditional generative model generates examples from a
          requested class.
        </li>
        <li>
          A feedforward style transfer network maps the content image it
          receives as input to a style-specific stylization.
        </li>
        <li>
          A visual question-answering (VQA) model processes an image within the
          context of a natural language question.
        </li>
        <li>
          <span class="todo"> TODO: add more examples.</span>
        </li>
      </ul>
      <p>
        Despite the different problem settings being solved, a lot of work
        makes use of very similar conditioning methods which revolve around
        conditional feature-wise affine transformations. Moreover, the specific
        form of the conditioning method is oftentimes accessory to the proposed
        innovation as opposed to being at the center of the work. Because of
        that, these similarities can be overlooked when discussing related
        work. We feel that there is a need to recognize and outline these
        similarities, as well as provide a common language to discuss about
        these approaches.
      </p>
      <p>
        This article therefore aims at regrouping these practices under the
        same framework and providing insights into how these methods work. It
        is split into three parts:
      </p>
      <ol>
        <li>
          An exposition to the proposed framework and nomenclature, named
          <strong>F</strong>eature-w<strong>i</strong>se <strong>L</strong>inear
          <strong>M</strong>odulation (FiLM).
        </li>
        <li>
          A thorough review of the literature connected to the use of
          conditional feature-wise affine transformations.
        </li>
        <li>
          A series of empirical observations of the behavior and properties of
          FiLM layers, leading to a hypothesis for explaining how it manages to
          condition a neural network.
        </li>
      </ol>

      <h2>FiLM</h2>
      <p>
        Like mentioned above, conditional feature-wise affine transformations
        oftentimes appear in the literature in various form factors, and we
        currently lack a common vocabulary to describe and discuss these
        approaches.
      </p>
      <p>
        In line with <dt-cite key="perez2017film"></dt-cite>, we propose to use
        the term FiLM, for <strong>F</strong>eature-w<strong>i</strong>se
        <strong>L</strong>inear <strong>M</strong>odulation, to describe
        conditional feature-wise affine transformations. In this framework, a
        neural network is conditioned using FiLM, or FiLM-ed, by inserting FiLM
        layers at various stages of the network's hierarchy.
      </p>
      <p>
        A FiLM layer can be thought of as a feature-wise linear layer--that is,
        affine transformations are applied elementwise, or in the case of
        convolutional networks, feature map-wise--whose scaling and shifting
        coefficients are a function of the conditioning information. In other
        words,

        \[
            \textrm{FiLM}(\mathbf{x}) = \gamma(\mathbf{c}) \odot \mathbf{x}
                                                           + \beta(\mathbf{c}),
        \]

        where \(\mathbf{x}\) is the FiLM layer's input and \(\mathbf{c}\) is the
        conditioning information. The mapping from conditioning information to
        FiLM parameters is called the FiLM generator. It receives \(\mathbf{c}\)
        as input and outputs \(\gamma\) and \(\beta\):

        \[
          \textrm{FiLM}_{\textrm{gen}}(\mathbf{c}) = [\gamma(\mathbf{c}),
                                                      \beta(\mathbf{c})].
        \]
      </p>
      <p>
        You can examine the relationship between the various FiLM components in
        the following diagram:
      </p>
      <div class="l-body-outset figure" id="film-diagram"></div>

      <h2>Relationship with other frameworks</h2>

      <h3>Bilinear transformations</h3>
      <p>
        In their seminal paper <dt-cite key="tenenbaum2000separating"></dt-cite>,
        Tenenbaum Freeman propose a learning algorithm for bilinear models based
        on SVD and EM.
      </p>
      <p>
        Let \(x\) and \(z\) be \(N\)- and \(M\)-dimensional feature vectors. A
        bilinear model defines the \(K\)-dimensional output vector as

          \[y_k = x^T W_k z\]

        where \(W_k\) is a \(N \times M\) matrix. Note that for each dimension
        \(k\) we have a separate matrix \(W_k\), so the weight matrix is a
        3-tensor.
      </p>
      <p>
        FiLM can be thought of as a particular rank-1 approximation to \(W_k\),
        namely

          \[y_k = x^T 1_k w_k^T z,\]

        where \(1_k\) is a one-hot vector with its \(k\)-th entry set to one.
        This approximation further assumes that \(\beta\) is fixed and set to
        zero, and that the FiLM projection layer does not include a bias.
      </p>
      <div class="l-body-outset figure" id="bilinear-diagram"></div>

      <h3>Spatial and temporal attention</h3>
      <p>
        In a sense, FiLM can be interpreted as a relaxed form of attention over
        features, which bears resemblance with attention over time or over
        space.
      </p>
      <p><span class="todo"> TODO: write about attention-related papers.</span></p>
      <p><span class="todo"> TODO: write about connection between attention and bilinear models.</span></p>

      <h3>HyperNetworks</h3>
      <p>
        FiLM shares with HyperNetworks <dt-cite key="ha2016hypernetworks"></dt-cite>
        the idea of using one neural network to predict parameters of another
        neural network. FiLM can be viewed as a specialized instance of a
        HyperNetwork which predicts the subset of the network's parameters
        contained in the FiLM layers.
      </p>
      <div class="l-body-outset figure" id="hypernetwork-diagram"></div>

      <h2>Conditional feature-wise affine transformations in the literature</h2>
      <p>
        We now turn our attention to the machine learning literature to expose
        the various ways in which conditional feature-wise affine
        transformations are used in practice. When appropriate, we will point
        out how these different ideas can be expressed using the FiLM
        nomenclature.
      </p>

      <h3>Visual question-answering (VQA) and visual reasoning</h3>
      <p>
        Work in <dt-cite key="perez2017learning"></dt-cite> and its expanded
        version <dt-cite key="perez2017film"></dt-cite> introduces the FiLM
        framework and ties together much prior FiLM-related work. Here, a visual
        reasoning model is trained on the CLEVR
        <dt-cite key="johnson2017clevr"></dt-cite> synthetic dataset to answer
        textual questions about an input image.
      </p>
      <div class="l-body-outset figure" id="clevr-diagram"></div>
      <p>
        Work in <dt-cite key="vries2017modulating"></dt-cite> introduced
        conditional batch normalization (CBN) for multimodal inputs in the
        context of VQA, more specifically the "oracle" sub-task of the
        GuessWhat?! task, which features natural language questions on images
        which are answered by yes, no, or N/A.
      </p>
      <p>
        The language pipeline is used to modulate the pre-trained image feature
        extractor by changing the values of the image feature extractor's batch
        normalization parameters. This paper also provides initial intuitions
        about FiLM by pointing out that conditioning the feature-wise affine
        transformation always results in better results than classic finetuning.
      </p>
      <div class="l-body-outset figure" id="guesswhat-diagram"></div>
      <p>
        CBN can be viewed as an instance of FiLM where the post-normalization
        feature-wise affine transformation is disabled and a FiLM layer is
        placed immediately after the batch normalization layer.
      </p>
      <p><span class="todo"> TODO: Talk about conditioning in relation networks.</span></p>

      <h3><a id="style-transfer">Artistic style transfer</a></h3>
      <p>
        On the artistic style transfer front,
        <dt-cite key="dumoulin2017learned"></dt-cite> and its successor
        <dt-cite key="ghiasi2017exploring"></dt-cite> implement a specific
        instantiation of FiLM.
      </p>
      <p>
        The former extends fast feedforward style transfer networks to multiple
        styles by introducing conditional instance normalization layers. Each
        style modeled by the network is associated with its own set of instance
        normalization parameters, and conditioning is achieved by assigning
        instance normalization parameters their corresponding value for the
        desired style. The latter introduces a <em>style prediction
        network</em> which is trained jointly with the style transfer network
        to predict the instance normalization parameters directly from the style
        image.
      </p>
      <div class="l-body-outset figure" id="alrfas-diagram"></div>
      <p>
        Both can be seen as an instantiation of FiLM in which FiLM layers
        replace the feature-wise affine transformation step of instance
        normalization layers and the FiLM-generating network is in the form of
        an embedding lookup (former work) or a parametrized mapping (latter
        work).
      </p>
      <p>
        Work in <dt-cite key="huang2017arbitrary"></dt-cite> proposes an
        alternative for fast and arbitrary style transfer in the form of
        adaptive instance normalization (AdaIN) layers, which perform instance
        normalization on a stak of content feature maps and scale and shift
        them using statistics extracted from a stack of style feature maps.
      </p>
      <p>
        Content and style images are fed through a trained classifier up to
        some intermediate layer, at which point the stack of content feature
        maps is instance-normalized and a feature-wise affine transformation
        is applied to them using the style instance normalization statistics. A
        decoder then maps the stack of content feature maps back to input
        space, and the triplet of content-style-stylized images are used to
        compute the usual style transfer loss.
      </p>
      <div class="l-body-outset figure" id="adain-diagram"></div>
      <p>
        AdaIN can be recognized as a FiLM layer replacing the instance
        normalization parameters, with the FiLM-generating network taking the
        form of the same trained classifier being used to extract instance
        normalization statistics on the style image.
      </p>

      <h3>Class-conditional generative modeling</h3>
      <p>
        Several models in the recent generative modeling literature have a
        class-conditional variant which leverages conditional feature-wise
        affine transformations in a way that is very reminiscent to FiLM
        layers.
      </p>
      <p>
        DCGAN <dt-cite key="radford2016unsupervised"></dt-cite>, a
        well-recognized class of network architectures for generative
        adversarial networks (GANs), lends itself to class conditioning by
        concatenating the class label broadcasted as a feature map to the input
        of convolutional and transposed convolutional layers in the generator
        and discriminator networks. An equivalent &mdash; and more efficient
        &mdash; way to express this operation is in the form of a feature-wise,
        class-conditional biasing of the output of the convolutional or
        transposed convolutional layer &mdash; ignoring the border effects due
        to zero padding. We can therefore view the class-conditional version of
        DCGAN as inserting FiLM layers after each convolutional and transposed
        convolutional layer. The FiLM-generating network comes in the form of
        an embedding lookup on the class labels, with the scaling coefficients
        set to a constant value of 1.
      </p>
      <div class="l-body-outset figure" id="dcgan-diagram"></div>
      <p>
        PixelCNN <dt-cite key="oord2016conditional"></dt-cite> and WaveNet
        <dt-cite key="oord2016wavenet"></dt-cite>, two recent advances in
        autoregressive modeling of image and audio data, respectively, employ a
        similar conditional biasing scheme. The inner workings of their
        autoregressive formulation is beyond the scope of this article, but
        their conditioning scheme can be explained from the FiLM perspective. 
      </p>
      <p>
        In its simplest form, conditioning in PixelCNN models is achieved by
        adding feature-wise, class-conditional biases to the output of all
        convolutional layers. Like for DCGAN, this can be viewed as inserting
        FiLM layers after each convolutional layer and expressing the
        FiLM-generating network as an embedding lookup on the class labels,
        with the scaling coefficients set to a constant value of 1. The authors
        also describe a location-dependent biasing scheme which cannot be
        expressed in terms of FiLM layers due to the absence of the
        feature-wise property.
      </p>
      <div class="l-body-outset figure" id="pixelcnn-diagram"></div>
      <p>
        WaveNet describes two ways in which conditional biasing allows external
        information to modulate the behavior of the model:
      </p>
      <ol>
        <li>
          <strong>Global conditioning</strong> applies the same conditional
          bias to the whole generated sequence and is used e.g. to condition on
          speaker identity.
        </li>
        <li>
          <strong>Local conditioning</strong> applies a conditional bias which
          varies across time steps of the generated sequence and is used e.g.
          to let linguistic features in a text-to-speech (TTS) model influence
          which sounds are produced.
        </li>
      </ol>
      <div class="l-body-outset figure" id="wavenet-diagram"></div>
      <p>
        Like DCGAN and PixelCNN, both approaches can be viewed as inserting
        FiLM layers after each convolutional layer. The main difference lies in
        the way in which the FiLM-generating network is defined: global
        conditioning expresses the FiLM-generating network as an embedding
        lookup which is broadcasted to the whole time series, whereas local
        conditioning expresses it as a mapping from an input sequence of
        conditioning information to an output sequence of FiLM parameters.
      </p>
      <p><span class="todo"> TODO: make a note about the use of gated convolutional layers in PixelCNN and WaveNet.</span></p>

      <h3>Self-conditioning</h3>
      <p>
        So far the applications we have considered condition on <em>external</em>
        information, but it is also possible (and sometimes desireable) to
        condition on <em>internal</em> information &mdash; that is, to
        <em>self-condition</em> a network.
      </p>
      <p>
        In the speech recognition domain, <dt-cite key="kim2017dynamic"></dt-cite>
        modulates a deep bidirectional LSTM using a form of conditional
        normalization which the authors call <em>dynamic layer normalization</em>.
        In analogy to conditional instance normalization and conditional batch
        normalization, dynamic layer normalization can be seen as an instance of
        FiLM where the post-normalization feature-wise affine transformation is
        disabled and a FiLM layer is placed immediately after the instance
        normalization layer.
      </p>
      <div class="l-body-outset figure" id="dln-diagram"></div>
      <p>
        The key difference here is that the conditioning signal does not come
        from an external source, but is rather computed from utterance
        summarization feature vectors extracted in each layer of the network
        as a way of adapting the neural acoustic model.
      </p>
      <p>
        In the visual domain, the ImageNet 2017 winning model
        <dt-cite key="hu2017squeeze"></dt-cite> employs a self-conditioning
        scheme in the form of feature-wise sigmoidal gating as a way to
        condition a layer's activations on its previous layer.
      </p>
      <div class="l-body-outset figure" id="squeeze-diagram"></div>
      <p>
        In the natural language domain, <dt-cite key="gehring2017convolutional"></dt-cite>
        introduces a fast and parallelizable model for machine translation in
        the form of a purely convolutional architecture. Despite not being the
        main technical innovation, this model also uses feature-wise sigmoidal
        gating to condition a layer's activations on its previous
        layer.
      </p>
      <div class="l-body-outset figure" id="convseq2seq-diagram"></div>
      <p>
        From the FiLM point of view, feature-wise sigmoidal gating is
        equivalent to a FiLM layer with the scaling being passed through a
        sigmoidal non-linearity and the shifting set to 0.
      </p>
      <p>
        The LSTM <dt-cite key="hochreiter1997long"></dt-cite> architecture
        itself can be viewed from the perspective of self-conditioned
        feature-wise affine transformations: the input, forget, and output gates
        can be seen as FiLM layers with a sigmoid-constrained \(\gamma\) and
        with \(\beta = 0\).
      </p>
      <div class="l-body-outset figure" id="lstm-diagram"></div>

      <h3>Attention over features</h3>
      <p>
        Work in <dt-cite key="dhingra2017gated"></dt-cite> introduces the
        Gated-Attention Reader, which conditions a document reading network
        with an associated query via multiple steps of feature-wise sigmoidal
        gating, and work in <dt-cite key="chaplot2017gated"></dt-cite> trains a
        reinforcement learning (RL) agent to follow simple natural language
        instructions in a 3D environment (VizDoom) using the same feature-wise
        sigmoidal gating mechanism. Both papers show improvement over
        conditioning via concatenation.
      </p>
      <div class="l-body-outset figure" id="gated-attention-diagram"></div>
      <p>
        Feature-wise sigmoidal gating can be seen as inserting a FiLM layer
        with scaling constrained to be between 0 and 1 via a sigmoidal layer
        and shifting set to zero.
      </p>
      <p>
        Work in <dt-cite key="shen2017disan"></dt-cite> proposes a feature-wise
        attention mechanism for language understanding, showing strong results
        on datasets such as SNLI <dt-cite key="bowman2015large"></dt-cite>:
      </p>
      <p>
        <em>Word embedding usually suffers from the polysemy in natural language.
        Since traditional attention computes a single importance score for
        each word based on the word embedding, it cannot distinguish the
        meanings of the same word in different contexts. Multi-dimensional
        attention, however, computes a score for each feature of each word, so
        it can select the features that can best describe the word’s specific
        meaning in any given context, and include this information in the
        sentence encoding outputs. </em>
        <span class="todo"> TODO: this is verbatim, we need to change it.</span>
      </p>
      <p></p>
      <div class="l-body-outset figure" id="disan-diagram"></div>

      <h3>Relationship with gating and mixture-of-experts</h3>
      <p>
        Work on mixture-of-experts models
        <dt-cite key="jacobs1991adaptive,jordan1994hierarchical,eigen2014deep"></dt-cite>
        also exhibit a strong relationship with conditional feature-wise affine
        transformations.
      </p>
      <p><span class="todo"> TODO: explain the relationship.</span></p>
      <div class="l-body-outset figure" id="moe-diagram"></div>

      <h3>Application to the reinforcement learning setting</h3>
      <p>
        In the reinforcement learning setting, work in
        <dt-cite key="kirkpatrick2017overcoming"></dt-cite> briefly touches the
        idea of applying conditional feature-wise affine transformations to
        reinforcement learning problems by training an agent that learns to play
        10 Atari games using a single Double DQN network with what can be
        thought of as FiLM layers throughout its hierarchy, conditioned on the
        current game.
      </p>
      <div class="l-body-outset figure" id="rl-diagram"></div>

      <h2>Feature-wise modulation in the neuroscience literature</h2>
      <p>
        Although attempts to connect deep learning results to the way the brain
        works should always be treated carefully, it is interesting to note that
        the idea of feature-wise modulation finds a modest amount of support in
        the neuroscience literature.
      </p>
      <p>
        In an attempt to explore biologically plausible neuron models while
        drawing a parallel with multi-layer perceptrons,
        <dt-cite key="mel1992clusteron"></dt-cite> introduces the
        <em>Clusteron</em>, a neuron whose activations can be amplified by
        surrounding neurons. This behavior enables to extract non-linearity and
        high-order statistics from data.
      </p>
      <p>
        Work in <dt-cite key="boutonnet2015words"></dt-cite> provides new
        evidence that external language cues alter how visual information is
        processed in the brain. More precisely, it is observed that P1 signals,
        which are related to low-level visual features, are modulated while
        hearing specific words.
      </p>

      <h2><a id="understanding">Understanding FiLM</a></h2>
      <p><span class="todo"> TODO: write a section on how FiLM operates.</span></p>
      <div class="l-body-outset figure" id="clevr-tsne"></div>

      <h2>Going forward</h2>
      <p><span class="todo"> TODO: write an opening statement on where to go next.</span></p>
    </dt-article>

    <dt-appendix>
      <h3>Acknowledgements</h3>
      <p><span class="todo"> TODO: WRITEME.</span><p>

    <script type="text/bibliography">
      @inproceedings{vries2017modulating,
        author={de Vries, Harm and Strub, Florian and Mary, J\'{e}r\'{e}mie and
                Larochelle, Hugo and Pietquin, Olivier and Courville, Aaron},
        title={Modulating early visual processing by language},
        booktitle={Advances in Neural Information Processing Systems},
        year={2017},
        url={https://arxiv.org/pdf/1707.00683.pdf},
      }
      @inproceedings{perez2017learning,
        author={Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin,
                Vincent and Courville, Aaron},
        title={Learning visual reasoning without strong priors},
        booktitle={ICML Workshop on Machine Learning in Speech and Language Processing},
        year={2017},
        url={https://arxiv.org/pdf/1707.03017.pdf},
      }
      @inproceedings{perez2017film,
        author={Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin,
                Vincent and Courville, Aaron},
        title={FiLM: Visual Reasoning with a General Conditioning Layer},
        booktitle={arXiv},
        year={2017},
        url={https://arxiv.org/pdf/1709.07871.pdf},
      }
      @inproceedings{johnson2017clevr,
        author={Johnson, Justin and Li, Fei-Fei and Hariharan, Bharath and Zitnick,
                Lawrence C. and van der Maaten, Laurens and Girshick, Ross},
        title={FiLM: Visual Reasoning with a General Conditioning Layer},
        booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition},
        year={2017},
        url={https://arxiv.org/pdf/1612.06890.pdf},
      }
      @inproceedings{dumoulin2017learned,
        author={Dumoulin, Vincent and Shlens, Jonathon and Kudlur, Manjunath},
        title={A Learned Representation for Artistic Style},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2017},
        url={https://arxiv.org/pdf/1610.07629.pdf},
      }
      @inproceedings{ghiasi2017exploring,
        author={Ghiasi, Golnaz and Lee, Honglak and Kudlur, Manjunath and
                Dumoulin, Vincent and Shlens, Jonathon},
        title={Exploring the structure of a real-time, arbitrary neural artistic
               stylization network},
        booktitle={Proceedings of the British Machine Vision Conference},
        year={2017},
        url={https://arxiv.org/pdf/1705.06830.pdf},
      }
      @inproceedings{huang2017arbitrary,
        title={Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},
        author={Huang, Xun and Belongie, Serge},
        booktitle={Proceedings of the International Conference on Computer Vision},
        year={2017},
        url={https://arxiv.org/pdf/1703.06868.pdf},
      }
      @inproceedings{radford2016unsupervised,
        author={Radford, Alec and Metz, Luke and Chintala, Soumith},
        title={Unsupervised Representation Learning with Deep Convolutional
               Generative Adversarial Networks},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2016},
        url={https://arxiv.org/pdf/1511.06434.pdf},
      }
      @inproceedings{oord2016conditional,
        title={Conditional Image Generation with PixelCNN Decoders},
        author={van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse
                and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
        booktitle={Advances in Neural Information Processing Systems},
        year={2016},
        url={https://arxiv.org/pdf/1606.05328.pdf}
      }
      @article{oord2016wavenet,
        title={WaveNet: A Generative Model for Raw Audio},
        author={van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and
                Simonyan, Karen and Vinyals, Oriol and Graves, Alex and
                Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
        journal={arXiv},
        year={2016},
        url={https://arxiv.org/pdf/1609.03499.pdf},
      }
      @inproceedings{ha2016hypernetworks,
        author={Ha, David and Dai, Andrew and Le, Quoc},
        title={HyperNetworks},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2016},
        url={https://arxiv.org/pdf/1609.09106.pdf},
      }
      @inproceedings{kim2017dynamic,
        author={Kim, Taesup and Song, Inchul and Bengio, Yoshua},
        title={Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling
               in Speech Recognition},
        booktitle={Interspeech},
        year={2017},
        url={https://arxiv.org/pdf/1707.06065.pdf},
      }
      @inproceedings{hu2017squeeze,
        author={Hu, Jie and Shen, Li and Sun, Gang},
        title={Squeeze-and-Excitation Networks},
        booktitle={CVPR's ILSVRC 2017 Workshop},
        year={2017},
        url={https://arxiv.org/pdf/1709.01507.pdf},
      }
      @inproceedings{gehring2017convolutional,
        author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats,
                Denis and Dauphin, Yann N.},
        title={Convolution Sequence-to-Sequence Learning},
        booktitle={Proceedings of the International Conference on Machine Learning},
        year={2017},
        url={https://arxiv.org/pdf/1705.03122.pdf},
      }
      @article{hochreiter1997long,
        title={Long Short-Term Memory},
        author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
        journal={Neural Computation},
        volume={9},
        number={8},
        pages={1735--1780},
        year={1997},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1997.9.8.1735},
      }
      @article{tenenbaum2000separating,
        title={Separating Style and Content with Bilinear Models},
        author={Tenenbaum, Joshua B. and Freeman, William T.},
        journal={Neural Computation},
        volume={12},
        number={6},
        pages={1247--1283},
        year={2000},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/089976600300015349},
      }
      @article{dhingra2017gated,
        title={Gated-Attention Readers for Text Comprehension},
        author={Dhingra, Bhuwan and Liu, Hanxiao and Yang, Zhilin, and
                Cohen, William W and Salakhutdinov, Ruslan},
        booktitle={Proceedings of the Annual Meeting of the Association for
                   Computational Linguistics},
        year={2017},
        url={https://arxiv.org/pdf/1606.01549.pdf},
      }
      @inproceedings{chaplot2017gated,
        title={Gated-Attention Architectures for Task-Oriented Language Grounding},
        author={Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and
                Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov,
                Ruslan},
        booktitle={ACL Workshop on Language Grounding for Robotics},
        year={2017},
        url={https://arxiv.org/pdf/1706.07230.pdf},
      }
      @inproceedings{shen2017disan,
        title={DiSAN: Directional Self-Attention Network for RNN/CNN-free
               Language Understanding},
        author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and
                Pan, Shirui and Zhang, Chengqi},
        booktitle={arXiv},
        year={2017},
        url={https://arxiv.org/pdf/1709.04696.pdf},
      }
      @inproceedings{bowman2015large,
        title={A large annotated corpus for learning natural language inference},
        author={Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and
                Manning, Christopher D.},
        booktitle={Proceedings of the Conference on Empirical Methods in Natural
                   Language Processing},
        year={2015},
        url={https://nlp.stanford.edu/pubs/snli_paper.pdf},
      }
      @article{jacobs1991adaptive,
        title={Adaptive Mixtures of Local Experts},
        author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J.
                and Hinton, Geoffrey E.},
        journal={Neural Computation},
        volume={3},
        number={1},
        pages={79--87},
        year={1991},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1991.3.1.79},
      }
      @article{jordan1994hierarchical,
        title={Hierarchical Mixtures of Experts and the EM Algorithm},
        author={Jordan, Michael I. and Jacobs, Robert A.},
        journal={Neural Computation},
        volume={6},
        number={2},
        pages={181--214},
        year={1994},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1994.6.2.181},
      }
      @inproceedings{eigen2014deep,
        author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
        title={Learning Factored Representations in a Deep Mixture of Experts},
        booktitle={Proceedings of the ICLR Workshops},
        year={2014},
        url={https://arxiv.org/pdf/1312.4314.pdf},
      }
      @article{kirkpatrick2017overcoming,
        title={Overcoming catastrophic forgetting in neural networks},
        author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and
                Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and
                Milan, Kieran and Quan, John and Ramalho, Tiago and
                Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath,
                Claudia and Kumaran, Dharshan and Hadsell, Raia},
        journal={Proceedings of the National Academy of Sciences},
        volume={114},
        number={13},
        pages={3521--3526},
        year={2017},
        url={http://www.pnas.org/content/114/13/3521.abstract},
      }
      @inproceedings{mel1992clusteron,
        title={The Clusteron: Toward a Simple Abstraction for a Complex Neuron},
        author={Mel, Bartlett W},
        booktitle={Advances in Neural Information Processing Systems},
        year={1992},
        url={http://papers.nips.cc/paper/450-the-clusteron-toward-a-simple-abstraction-for-a-complex-neuron.pdf},
      }
      @article{boutonnet2015words,
        title={Words jump-start vision: A label advantage in object recognition},
        author={B. Boutonnet and G. Lupyan},
        journal={Journal of Neuroscience},
        volume={35},
        number={25},
        pages={9329--9335},
        year={2015},
        publisher={Society for Neuroscience},
        url={},
      }
    </script>
    <script src="assets/figure_film.js"></script>
    <script src="assets/figure_film_mlp.js"></script>
    <script src="assets/figure_film_cnn.js"></script>
    <script src="assets/figure_film_architecture.js"></script>
    <script src="assets/figure_bilinear.js"></script>
    <script src="assets/figure_hypernetwork.js"></script>
    <script src="assets/figure_clevr.js"></script>
    <script src="assets/figure_guesswhat.js"></script>
    <script src="assets/figure_alrfas.js"></script>
    <script src="assets/figure_adain.js"></script>
    <script src="assets/figure_dcgan.js"></script>
    <script src="assets/figure_pixelcnn.js"></script>
    <script src="assets/figure_wavenet.js"></script>
    <script src="assets/figure_dln.js"></script>
    <script src="assets/figure_squeeze.js"></script>
    <script src="assets/figure_convseq2seq.js"></script>
    <script src="assets/figure_lstm.js"></script>
    <script src="assets/figure_gated_attention.js"></script>
    <script src="assets/figure_disan.js"></script>
    <script src="assets/figure_moe.js"></script>
    <script src="assets/figure_rl.js"></script>
    <script src="assets/figure_clevr_tsne.js"></script>
  </body>
</html>
