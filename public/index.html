<!doctype html>
<meta charset="utf-8">
<link href="assets/style.css" type="text/css" rel="stylesheet">
<script src="https://distill.pub/template.v1.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script src="https://d3js.org/d3.v4.min.js"></script>

<script type="text/front-matter">
  title: "Understanding FiLM"
  description: "Description of the post"
  authors:
  - Vincent Dumoulin: https://vdumoulin.github.io
  - Ethan Perez: http://ethanperez.net/
  - Florian Strub: https://fstrub95.github.io/
  - Harm de Vries: http://www-etud.iro.umontreal.ca/~devries/
  - Aaron Courville: https://aaroncourville.wordpress.com/
  - Yoshua Bengio: http://www.iro.umontreal.ca/~bengioy/yoshua_en/
  affiliations:
  - MILA: https://mila.quebec/en/
  - Rice University: http://www.rice.edu/
  - Inria SequeL: https://team.inria.fr/sequel/
  - MILA: https://mila.quebec/en/
  - MILA: https://mila.quebec/en/
  - MILA: https://mila.quebec/en/
</script>

<dt-article>
  <h1>Understanding feature-wise linear modulation (FiLM) layers</h1>
  <h2><span class="todo"> TODO: Write a description of the article.</span></h2>

  <dt-byline></dt-byline>

  <h2>Introduction</h2>
  <p><span class="todo"> TODO: write an introduction.</span></p>
  <p>We can also cite <dt-cite key="perez2017film"></dt-cite> external publications.</p>

  <h2>FiLM layers</h2>
  <p><span class="todo"> TODO: Explain FiLM.</span></p>

  <h2>Related work</h2>
  <p>
    We now turn our attention to the machine learning literature to point out
    instances of the FiLM framework being used in the wild, explicitly or
    implicitly. The objective is to show that a number of existing models can
    be cast as an instantiation of FiLM, which allows us to reason about these
    methods in a unified framework.
  </p>

  <h3>Visual question-answering (VQA) and visual reasoning</h3>
  <p>
    Work in <dt-cite key="perez2017learning"></dt-cite> and its expanded
    version <dt-cite key="perez2017film"></dt-cite> introduces the FiLM
    framework and ties together much prior FiLM-related work. Here, a visual
    reasoning model is trained on the CLEVR <dt-cite key="johnson2017clevr"></dt-cite>
    synthetic dataset to answer textual questions about an input image.
  </p>
  <div class="l-body-outset figure" id="film-clevr"><img src="assets/film_clevr.svg"></img></div>
  <p><span class="todo"> TODO: write about "Modulating early visual processing by language".</span></p>

  <h3>Artistic style transfer</h3>
  <p>
    On the artistic style transfer front, <dt-cite key="dumoulin2017learned"></dt-cite>
    and its successor <dt-cite key="ghiasi2017exploring"></dt-cite> implement a
    specific instantiation of FiLM.
  </p>
  <p>
    The former extends fast feedforward style transfer networks to multiple
    styles by introducing conditional instance normalization layers. Each style
    modeled by the network is associated with its own set of instance
    normalization parameters, and conditioning is achieved by assigning
    instance normalization parameters their corresponding value for the desired
    style. The latter introduces a <em>style prediction network</em> which is
    trained jointly with the style transfer network to predict the instance
    normalization parameters directly from the style image.
  </p>
  <p>
    Both can be seen as an instantiation of FiLM in which FiLM layers replace
    the affine transformation step of instance normalization layers and the
    FiLM-generating network is in the form of an embedding lookup (former work)
    or a parametrized mapping (latter work).
  </p>
  <p>
    Work in <dt-cite key="huang2017arbitrary"></dt-cite> proposes an
    alternative for fast and arbitrary style transfer in the form of adaptive
    instance normalization (AdaIN) layers, which perform instance normalization
    on a stak of content feature maps and scale and shift them using statistics
    extracted from a stack of style feature maps.
  </p>
  <p>
    Content and style images are fed through a trained classifier up to some
    intermediate layer, at which point the stack of content feature maps is
    instance-normalized and an affine feature-wise transformation is applied to
    them using the style instance normalization statistics. A decoder then maps
    the stack of content feature maps back to input space, and the triplet of
    content-style-stylized images are used to compute the usual style transfer
    loss.
  </p>
  <p>
    AdaIN can be recognized as a FiLM layer replacing the instance
    normalization parameters, with the FiLM-generating network taking the form
    of the same trained classifier being used to extract instance normalization
    statistics on the style image.
  </p>

  <h3>Class-conditional generative modeling</h3>
  <p>
    Several models in the recent generative modeling literature have a
    class-conditional variant which leverages conditional feature-wise affine
    transformations in a way that is very reminiscent to FiLM layers.
  </p>
  <p>
    DCGAN <dt-cite key="radford2016unsupervised"></dt-cite>, a well-recognized
    class of network architectures for generative adversarial networks (GANs),
    lends itself to class conditioning by concatenating the class label
    broadcasted as a feature map to the input of convolutional and transposed
    convolutional layers in the generator and discriminator networks. An
    equivalent &mdash; and more efficient &mdash; way to express this operation
    is in the form of a feature-wise, class-conditional biasing of the output
    of the convolutional or transposed convolutional layer &mdash; ignoring the
    border effects due to zero padding. We can therefore view the
    class-conditional version of DCGAN as inserting FiLM layers after each
    convolutional and transposed convolutional layer. The FiLM-generating
    network comes in the form of an embedding lookup on the class labels, with
    the scaling coefficients set to a constant value of 1.
  </p>
  <p>
    PixelCNN <dt-cite key="oord2016conditional"></dt-cite> and WaveNet
    <dt-cite key="oord2016wavenet"></dt-cite>, two recent advances in
    autoregressive modeling of image and audio data, respectively, employ a
    similar conditional biasing scheme. The inner workings of their
    autoregressive formulation is beyond the scope of this article, but their
    conditioning scheme can be explained from the FiLM perspective. 
  </p>
  <p>
    The conditional variant of PixelCNN 
    <dt-cite key="oord2016wavenet"></dt-cite>, two recent advances in
    autoregressive modeling of image and audio data, respectively, employ a
    similar conditional biasing scheme. The inner workings of their
    autoregressive formulation is beyond the scope of this article, but their
    conditioning scheme can be explained from the FiLM perspective. 
  </p>
  <p>
    In its simplest form, conditioning in PixelCNN models is achieved by adding
    feature-wise, class-conditional biases to the output of all convolutional
    layers. Like for DCGAN, this can be viewed as inserting FiLM layers after
    each convolutional layer and expressing the FiLM-generating network as an
    embedding lookup on the class labels, with the scaling coefficients set to
    a constant value of 1. The authors also describe a location-dependent
    biasing scheme which cannot be expressed in terms of FiLM layers due to the
    absence of the feature-wise property.
  </p>
  <p>
    WaveNet describes two ways in which conditional biasing allows external information
    to modulate the behavior of the model:
  </p>
  <ol>
    <li>
      <strong>Global conditioning</strong> applies the same conditional bias to
      the whole generated sequence and is used e.g. to condition on speaker
      identity.
    </li>
    <li>
      <strong>Local conditioning</strong> applies a conditional bias which
      varies across time steps of the generated sequence and is used e.g. to
      let linguistic features in a text-to-speech (TTS) model influence which
      sounds are produced.
    </li>
  </ol>
  <p>
    Like DCGAN and PixelCNN, both approaches can be viewed as inserting FiLM
    layers after each convolutional layer. The main difference lies in the way
    in which the FiLM-generating network is defined: global conditioning
    expresses the FiLM-generating network as an embedding lookup which is
    broadcasted to the whole time series, whereas local conditioning expresses
    it as a mapping from an input sequence of conditioning information to an
    output sequence of FiLM parameters.
  </p>
  <p><span class="todo"> TODO: make a note about the use of gated convolutional layers in PixelCNN and WaveNet.</span></p>

  <h3>Self-conditioning</h3>
  <p><span class="todo"> TODO: write about dynamic layer normalization, squeeze-and-excitation networks, "Convolution Sequence-to-Sequence Learning", and LSTMs.</span></p>

  <h3>Relationship with HyperNetworks</h3>
  <p><span class="todo"> TODO: write about HyperNetworks.</span></p>

  <h3>Relationship with attention</h3>
  <p><span class="todo"> TODO: write about "Gated-Attention Readers for Text Comprehension", "Gated-Attention Architectures for Task-Oriented Language Grounding", and "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding".</span></p>

  <h3>Relationship with gating and mixture-of-experts</h3>
  <p><span class="todo"> TODO: write about "Adaptive Mixtures of Local Experts", "Hierarchical Mixtures of Experts and the EM Algorithm", and "Learning Factored Representations in a Deep Mixture of Experts".</span></p>

  <h3>Application to the reinforcement learning setting</h3>
  <p><span class="todo"> TODO: write about "Overcoming catastrophic forgetting in neural networks".</span></p>

  <h2>Understanding FiLM</h2>
  <p><span class="todo"> TODO: write a section on how FiLM operates.</span></p>

  <h2>Going forward</h2>
  <p><span class="todo"> TODO: write an opening statement on where to go next.</span></p>
</dt-article>

<dt-appendix>
  <h3>Acknowledgements</h3>
  <p><span class="todo"> TODO: WRITEME.</span><p>

  <h3>Author Contributions</h3>
  <p>Vincent Dumoulin <span class="todo"> TODO: WRITEME</span>.<p>
  <p>Ethan Perez <span class="todo"> TODO: WRITEME</span>.<p>
  <p>Florian Strub <span class="todo"> TODO: WRITEME</span>.<p>
  <p>Harm de Vries <span class="todo"> TODO: WRITEME</span>.<p>
  <p>Aaron Courville <span class="todo"> TODO: WRITEME</span>.<p>
  <p>Yoshua Bengio <span class="todo"> TODO: WRITEME</span>.<p>
</dt-appendix>

<script type="text/bibliography">
  @inproceedings{perez2017learning,
    author={Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin,
            Vincent and Courville, Aaron},
    title={Learning visual reasoning without strong priors},
    booktitle={ICML Workshop on Machine Learning in Speech and Language Processing},
    year={2017},
    url={https://arxiv.org/pdf/1707.03017.pdf},
  }
  @inproceedings{perez2017film,
    author={Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin,
            Vincent and Courville, Aaron},
    title={FiLM: Visual Reasoning with a General Conditioning Layer},
    booktitle={arXiv},
    year={2017},
    url={https://arxiv.org/pdf/1709.07871.pdf},
  }
  @inproceedings{johnson2017clevr,
    author={Johnson, Justin and Li, Fei-Fei and Hariharan, Bharath and Zitnick,
            Lawrence C. and van der Maaten, Laurens and Girshick, Ross},
    title={FiLM: Visual Reasoning with a General Conditioning Layer},
    booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition},
    year={2017},
    url={https://arxiv.org/pdf/1612.06890.pdf},
  }
  @inproceedings{dumoulin2017learned,
    author={Dumoulin, Vincent and Shlens, Jonathon and Kudlur, Manjunath},
    title={A Learned Representation for Artistic Style},
    booktitle={Proceedings of the International Conference on Learning Representations},
    year={2017},
    url={https://arxiv.org/pdf/1610.07629.pdf},
  }
  @inproceedings{ghiasi2017exploring,
    author={Ghiasi, Golnaz and Lee, Honglak and Kudlur, Manjunath and Dumoulin,
            Vincent and Shlens, Jonathon},
    title={Exploring the structure of a real-time, arbitrary neural artistic
           stylization network},
    booktitle={Proceedings of the British Machine Vision Conference},
    year={2017},
    url={https://arxiv.org/pdf/1705.06830.pdf},
  }
  @inproceedings{huang2017arbitrary,
    title={Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},
    author={Huang, Xun and Belongie, Serge},
    booktitle={Proceedings of the International Conference on Computer Vision},
    year={2017},
    url={https://arxiv.org/pdf/1703.06868.pdf},
  }
  @inproceedings{radford2016unsupervised,
    author={Radford, Alec and Metz, Luke and Chintala, Soumith},
    title={Unsupervised Representation Learning with Deep Convolutional
           Generative Adversarial Networks},
    booktitle={Proceedings of the International Conference on Learning Representations},
    year={2016},
    url={https://arxiv.org/pdf/1511.06434.pdf},
  }
  @inproceedings{oord2016conditional,
    title={Conditional Image Generation with PixelCNN Decoders},
    author={van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and
            Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
    booktitle={Advances in Neural Information Processing Systems},
    year={2016},
    url={https://arxiv.org/pdf/1606.05328.pdf}
  }
  @article{oord2016wavenet,
    title={WaveNet: {A} Generative Model for Raw Audio},
    author={van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan,
            Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and
            Senior, Andrew and Kavukcuoglu, Koray},
    journal={arXiv},
    year={2016},
    url={https://arxiv.org/pdf/1609.03499.pdf},
  }
</script>
