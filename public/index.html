<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <link href="assets/style.css" type="text/css" rel="stylesheet">
    <script src="https://distill.pub/template.v1.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
    <script src="https://d3js.org/d3.v4.min.js"></script>
    <script src="https://d3js.org/d3-selection-multi.v1.min.js"></script>

    <script type="text/front-matter">
      title: "Understanding FiLM"
      description: "Description of the post"
      authors:
      - Vincent Dumoulin: https://vdumoulin.github.io
      - Ethan Perez: http://ethanperez.net/
      - Florian Strub: https://fstrub95.github.io/
      - Harm de Vries: http://www-etud.iro.umontreal.ca/~devries/
      - Nathan Schucher: https://nathanschucher.com/
      - Aaron Courville: https://aaroncourville.wordpress.com/
      - Yoshua Bengio: http://www.iro.umontreal.ca/~bengioy/yoshua_en/
      affiliations:
      - MILA: https://mila.quebec/en/
      - Rice University: http://www.rice.edu/
      - Inria SequeL: https://team.inria.fr/sequel/
      - MILA: https://mila.quebec/en/
      - Element AI: https://element.ai/
      - MILA: https://mila.quebec/en/
      - MILA: https://mila.quebec/en/
    </script>
  </head>
  <body>
    <dt-article>
      <h1>Understanding feature-wise linear modulation (FiLM) layers</h1>
      <h2><span class="todo"> TODO: Write a description of the article.</span></h2>

      <dt-byline></dt-byline>

      <p>
        Much like attentional interfaces, which have been the subject of a
        previous <a href="https://distill.pub/2016/augmented-rnns/">Distill
        article</a>, feature-wise affine transformations have emerged as a
        powerful mechanism which allows to modulate the computation carried out
        by a neural network.
      </p>
      <p>
        To center the discussion around a common language, we propose to use
        the nomenclature of <dt-cite key="perez2017film"></dt-cite>, which
        calls these transformations <em>FiLM</em>, for
        <strong>F</strong>eature-w<strong>i</strong>se <strong>L</strong>inear
        <strong>M</strong>odulation.<dt-fn>Strictly speaking, <em>linear</em>
        is a misnomer, as we allow biasing, but we hope the more rigorous-minded
        reader will forgive us for the sake of a better-sounding acronym.</dt-fn>
      </p>
      <p>
        We say that a neural network is modulated using FiLM, or
        <em>FiLM-ed</em>, through the insertion of <em>FiLM layers</em> in its
        architecture. These are parametrized by some form of conditioning
        information, and the mapping from conditioning information to FiLM
        parameters is called the <em>FiLM generator</em>. For simplicity, you
        can assume that the FiLM generator outputs the concatenation of all
        FiLM parameters for the network architecture.
      </p>
      <figure class="l-body-outset figure" id="film-architecture-diagram">
        <figcaption>
          In this example, a network is FiLM-ed by inserting two FiLM layers in
          its architecture. The FiLM parameters are computed using the FiLM
          generator, which processes the conditioning information.
        </figcaption>
      </figure>
      <p>
        As the name implies, a FiLM layer applies a feature-wise affine
        transformation to its input. The parameters of that transformation are
        also provided as input. By <em>feature-wise</em>, we mean that scaling
        and shifting are applied elementwise, or in the case of convolutional
        networks, feature map-wise. In other words,

        <dt-math block>
            \textrm{FiLM}(\mathbf{x}) = \gamma(\mathbf{c}) \odot \mathbf{x}
                                                           + \beta(\mathbf{c}),
        </dt-math>

        where <dt-math>\mathbf{x}</dt-math> is the FiLM layer's input and
        <dt-math>\mathbf{c}</dt-math> is the conditioning information. You can
        interact with the following fully-connected and convolutional FiLM
        layers to get an intuition of the sort of modulation they allow:
      </p>
      <figure class="l-body-outset figure" id="film-diagram">
        <figcaption>
          A FiLM layers applied on a fully-connected input (left) and on a
          convolutional input (right). In both cases, the scaling and shifting
          vectors <dt-math>\gamma</dt-math> and <dt-math>\beta</dt-math> are
          passed as input and are computed by the FiLM generator. Try
          interacting with the <dt-math>\gamma</dt-math> and
          <dt-math>\beta</dt-math> values by hovering over their corresponding
          diagram elements and observe the resulting change in the output.
        </figcaption>
      </figure>
      <p>
        <span class="todo">TODO: say a few words about what this allows us to do
        (e.g., shutting off feature maps)</span>
      </p>
      <p>
        You may also think of FiLM as an efficient rank-1 approximation of a
        bilinear transformation <dt-cite key="tenenbaum2000separating"></dt-cite>.
        In general, a bilinear transformation is defined as

        <dt-math block>
          y_k = \mathbf{x}^T W_k \mathbf{z}
        </dt-math>

        where <dt-math>\mathbf{x}</dt-math> and <dt-math>\mathbf{z}</dt-math>
        are <dt-math>N</dt-math>- and <dt-math>M</dt-math>-dimensional feature
        vectors, <dt-math>y_k</dt-math> is the <dt-math>k^{th}</dt-math>
        output, and <dt-math>W_k</dt-math> is a <dt-math>N \times M</dt-math>
        matrix. Note that for each output <dt-math>y_k</dt-math> we have a
        separate matrix <dt-math>W_k</dt-math>, so the full set of weights
        forms a 3-tensor.
      </p>
      <figure class="l-body-outset figure" id="bilinear-diagram">
        <figcaption>
          Casting the FiLM layer as a low-rank approximation of a bilinear
          transformation. <strong>(Top)</strong> A bilinear transformation
          mapping <dt-math>\mathbf{x}</dt-math> and
          <dt-math>\mathbf{z}</dt-math> &mdash; both 3-dimensional &mdash; to a
          3-dimensional output. Each output element <dt-math>y_k</dt-math> is
          computed as a vector-matrix-vector product using a separate matrix
          <dt-math>W_k</dt-math>. <strong>(Bottom)</strong> We can achieve the
          feature-wise affine transformation carried out by FiLM using a
          bilinear transformation. The input <dt-math>\mathbf{x}</dt-math> is
          augmented with a <dt-math>1</dt-math>-valued constant feature, and all
          rows in the weight matrices <dt-math>W_k</dt-math> except the
          <dt-math>k^{th}</dt-math> row and the last row are zeroed out (shown
          in grey in the figure). The name of the input
          <dt-math>\mathbf{z}</dt-math> has been changed to
          <dt-math>f(\mathbf{c})</dt-math> to emphasize the fact that it depends
          on the conditioning information.
        </figcaption>
      </figure>
      <p>
        We can cast FiLM as a bilinear transformation by considering the
        equation

        <dt-math block>
          y_k = \tilde{\mathbf{x}}^T W_k f(\mathbf{c})
        </dt-math>

        where <dt-math>\tilde{\mathbf{x}}</dt-math> is the input
        <dt-math>\mathbf{x}</dt-math> augmented with a
        <dt-math>1</dt-math>-valued constant feature &mdash; a common trick to
        turn a linear transformation into an affine transformation &mdash; and
        <dt-math>\mathbf{z}</dt-math> has been replaced with
        <dt-math>f(\mathbf{c})</dt-math> to emphasize its role as a
        representation of the conditioning information.
      </p>
      <p>
        The weight matrix <dt-math>W_k</dt-math> is filled with zeros in such a
        way that

        <dt-math block>
          W_k f(\mathbf{c}) = (0, \ldots, 0, \gamma_k, 0, \ldots, 0, \beta_k)
        </dt-math>

        where <dt-math>\gamma_k</dt-math> and <dt-math>\beta_k</dt-math> are
        the FiLM scaling and shifting coefficients for the
        <dt-math>k^{th}</dt-math> feature, and <dt-math>\gamma_k</dt-math> is
        the <dt-math>k^{th}</dt-math> element of the vector.
      </p>
      <p>
        Interestingly, spatial and temporal attention can also be viewed as
        low-rank approximations of a bilinear transformation. <span
        class="todo"> TODO: elaborate.</span> Furthermore, FiLM itself can be
        interpreted as a relaxed form of attention over features.
      </p>
      <p>
        FiLM also shares with HyperNetworks <dt-cite key="ha2016hypernetworks"></dt-cite>
        the architectural feature that a secondary neural network computes some
        of the parameter values in the primary network. From that perspective,
        the FiLM generator can be viewed as a specialized instance of a
        HyperNetwork which predicts the FiLM parameters of the FiLM-ed network.
      </p>
      <p>
        Finally, although attempts to connect deep learning results to the way
        the brain works should always be treated carefully, it is interesting to
        note that the idea of feature-wise modulation finds a modest amount of
        support in the neuroscience literature.
      </p>
      <p>
        In an attempt to explore biologically plausible neuron models while
        drawing a parallel with multi-layer perceptrons,
        <dt-cite key="mel1992clusteron"></dt-cite> introduces the
        <em>Clusteron</em>, a neuron whose activations can be amplified by
        surrounding neurons. This behavior enables to extract non-linearity and
        high-order statistics from data.
      </p>
      <p>
        Work in <dt-cite key="boutonnet2015words"></dt-cite> provides new
        evidence that external language cues alter how visual information is
        processed in the brain. More precisely, it is observed that P1 signals,
        which are related to low-level visual features, are modulated while
        hearing specific words.
      </p>
      <p>
        In the spirit of the <a href="https://distill.pub/2016/augmented-rnns/">
        Distill article</a> on attention, we outlined four directions through
        which feature-wise affine transformations express themselves in the
        literature:
      </p>

      <figure class="l-body-outset previews">
        <a href="#conditioning">
          <img src="assets/preview_external.svg" />
          <figcaption>Conditioning on external information</figcaption>
        </a>
        <a href="#self-conditioning">
          <img src="assets/preview_self.svg" />
          <figcaption>Self-conditioning</figcaption>
        </a>
        <a href="#attention-over-features">
          <img src="assets/preview_attention.svg" />
          <figcaption>Attention over features</figcaption>
        </a>
        <a href="#gating">
          <img src="assets/preview_gating.svg" />
          <figcaption>Gating</figcaption>
        </a>
      </figure>

      <p>
        We also attempt to characterize and explain the behavior of FiLM-ed
        networks through analyses on toy and real-world tasks:
      </p>

      <figure class="l-body-outset previews">
        <a href="#film-as-task-embeddings">
          <img src="assets/preview_tsne.svg" />
          <figcaption>FiLM parameters as task embeddings</figcaption>
        </a>
      </figure>

      <style>
        .previews a {
          text-decoration: none;
          overflow: hidden;
          margin-bottom: 12px;
          display: block;
          border-bottom: 1px solid rgba(0, 0, 0, 0.1);
          margin-bottom: 12px;
          padding-bottom: 12px;
        }
        .previews figcaption {
          margin-left: 100px;
        }
        .previews svg, .previews img {
          width: 80px;
          float: left;
        }
        @media(min-width: 768px) {
          .previews {
            overflow: hidden;
            margin-top: 48px;
            margin-bottom: 48px;
          }
          .previews figcaption {
            margin-left: 0;
          }
          .previews figcaption b {
            display: block;
          }
          .previews figcaption b span {
            display: block;
          }
          .previews a {
            position: relative;
            float: left;
            width: 19%;
            margin-right: 3.6%;
            padding-right: 3.6%;
            border-right: 1px solid rgba(0, 0, 0, 0.05);
            border-bottom: none;
            margin-bottom: 0;
            padding-bottom: 0;
          }
          .previews a:last-child {
            margin-right: 0;
            padding-right: 0;
            border-right: 0;
          }
          .previews svg, .previews img {
            margin-bottom: 18px;
            display: block;
            width: 100%;
            float: none;
          }
        }
        @media(min-width: 1024px) {
          .previews {
            margin-top: 90px;
            margin-bottom: 90px;
          }
        }
      </style>

      <hr />

      <h1>Feature-wise affine transformations in the literature</h1>

      <h2 id="conditioning">Conditioning on external information</h2>
      <p>
        Work in <dt-cite key="perez2017learning"></dt-cite> and its expanded
        version <dt-cite key="perez2017film"></dt-cite> introduces the FiLM
        framework and ties together much prior FiLM-related work. Here, a visual
        reasoning model is trained on the CLEVR
        <dt-cite key="johnson2017clevr"></dt-cite> synthetic dataset to answer
        textual questions about an input image.
      </p>
      <div class="l-body-outset figure" id="clevr-diagram"></div>
      <p>
        Work in <dt-cite key="vries2017modulating"></dt-cite> introduced
        conditional batch normalization (CBN) for multimodal inputs in the
        context of VQA, more specifically the "oracle" sub-task of the
        GuessWhat?! task, which features natural language questions on images
        which are answered by yes, no, or N/A.
      </p>
      <p>
        The language pipeline is used to modulate the pre-trained image feature
        extractor by changing the values of the image feature extractor's batch
        normalization parameters. This paper also provides initial intuitions
        about FiLM by pointing out that conditioning the feature-wise affine
        transformation always results in better results than classic finetuning.
      </p>
      <div class="l-body-outset figure" id="guesswhat-diagram"></div>
      <p>
        CBN can be viewed as an instance of FiLM where the post-normalization
        feature-wise affine transformation is disabled and a FiLM layer is
        placed immediately after the batch normalization layer.
      </p>
      <p><span class="todo"> TODO: Talk about conditioning in relation networks.</span></p>
      <p>
        On the artistic style transfer front,
        <dt-cite key="dumoulin2017learned"></dt-cite> and its successor
        <dt-cite key="ghiasi2017exploring"></dt-cite> implement a specific
        instantiation of FiLM.
      </p>
      <p>
        The former extends fast feedforward style transfer networks to multiple
        styles by introducing conditional instance normalization layers. Each
        style modeled by the network is associated with its own set of instance
        normalization parameters, and conditioning is achieved by assigning
        instance normalization parameters their corresponding value for the
        desired style. The latter introduces a <em>style prediction
        network</em> which is trained jointly with the style transfer network
        to predict the instance normalization parameters directly from the style
        image.
      </p>
      <div class="l-body-outset figure" id="alrfas-diagram"></div>
      <p>
        Both can be seen as an instantiation of FiLM in which FiLM layers
        replace the feature-wise affine transformation step of instance
        normalization layers and the FiLM-generating network is in the form of
        an embedding lookup (former work) or a parametrized mapping (latter
        work).
      </p>
      <p>
        Work in <dt-cite key="huang2017arbitrary"></dt-cite> proposes an
        alternative for fast and arbitrary style transfer in the form of
        adaptive instance normalization (AdaIN) layers, which perform instance
        normalization on a stak of content feature maps and scale and shift
        them using statistics extracted from a stack of style feature maps.
      </p>
      <p>
        Content and style images are fed through a trained classifier up to
        some intermediate layer, at which point the stack of content feature
        maps is instance-normalized and a feature-wise affine transformation
        is applied to them using the style instance normalization statistics. A
        decoder then maps the stack of content feature maps back to input
        space, and the triplet of content-style-stylized images are used to
        compute the usual style transfer loss.
      </p>
      <div class="l-body-outset figure" id="adain-diagram"></div>
      <p>
        AdaIN can be recognized as a FiLM layer replacing the instance
        normalization parameters, with the FiLM-generating network taking the
        form of the same trained classifier being used to extract instance
        normalization statistics on the style image.
      </p>
      <p>
        Several models in the recent generative modeling literature have a
        class-conditional variant which leverages conditional feature-wise
        affine transformations in a way that is very reminiscent to FiLM
        layers.
      </p>
      <p>
        DCGAN <dt-cite key="radford2016unsupervised"></dt-cite>, a
        well-recognized class of network architectures for generative
        adversarial networks (GANs), lends itself to class conditioning by
        concatenating the class label broadcasted as a feature map to the input
        of convolutional and transposed convolutional layers in the generator
        and discriminator networks. An equivalent &mdash; and more efficient
        &mdash; way to express this operation is in the form of a feature-wise,
        class-conditional biasing of the output of the convolutional or
        transposed convolutional layer &mdash; ignoring the border effects due
        to zero padding. We can therefore view the class-conditional version of
        DCGAN as inserting FiLM layers after each convolutional and transposed
        convolutional layer. The FiLM-generating network comes in the form of
        an embedding lookup on the class labels, with the scaling coefficients
        set to a constant value of 1.
      </p>
      <div class="l-body-outset figure" id="dcgan-diagram"></div>
      <p>
        PixelCNN <dt-cite key="oord2016conditional"></dt-cite> and WaveNet
        <dt-cite key="oord2016wavenet"></dt-cite>, two recent advances in
        autoregressive modeling of image and audio data, respectively, employ a
        similar conditional biasing scheme. The inner workings of their
        autoregressive formulation is beyond the scope of this article, but
        their conditioning scheme can be explained from the FiLM perspective. 
      </p>
      <p>
        In its simplest form, conditioning in PixelCNN models is achieved by
        adding feature-wise, class-conditional biases to the output of all
        convolutional layers. Like for DCGAN, this can be viewed as inserting
        FiLM layers after each convolutional layer and expressing the
        FiLM-generating network as an embedding lookup on the class labels,
        with the scaling coefficients set to a constant value of 1. The authors
        also describe a location-dependent biasing scheme which cannot be
        expressed in terms of FiLM layers due to the absence of the
        feature-wise property.
      </p>
      <div class="l-body-outset figure" id="pixelcnn-diagram"></div>
      <p>
        WaveNet describes two ways in which conditional biasing allows external
        information to modulate the behavior of the model:
      </p>
      <ol>
        <li>
          <strong>Global conditioning</strong> applies the same conditional
          bias to the whole generated sequence and is used e.g. to condition on
          speaker identity.
        </li>
        <li>
          <strong>Local conditioning</strong> applies a conditional bias which
          varies across time steps of the generated sequence and is used e.g.
          to let linguistic features in a text-to-speech (TTS) model influence
          which sounds are produced.
        </li>
      </ol>
      <div class="l-body-outset figure" id="wavenet-diagram"></div>
      <p>
        Like DCGAN and PixelCNN, both approaches can be viewed as inserting
        FiLM layers after each convolutional layer. The main difference lies in
        the way in which the FiLM-generating network is defined: global
        conditioning expresses the FiLM-generating network as an embedding
        lookup which is broadcasted to the whole time series, whereas local
        conditioning expresses it as a mapping from an input sequence of
        conditioning information to an output sequence of FiLM parameters.
      </p>
      <p><span class="todo"> TODO: make a note about the use of gated convolutional layers in PixelCNN and WaveNet.</span></p>
      <p>
        In the reinforcement learning setting, work in
        <dt-cite key="kirkpatrick2017overcoming"></dt-cite> briefly touches the
        idea of applying conditional feature-wise affine transformations to
        reinforcement learning problems by training an agent that learns to play
        10 Atari games using a single Double DQN network with what can be
        thought of as FiLM layers throughout its hierarchy, conditioned on the
        current game.
      </p>
      <div class="l-body-outset figure" id="rl-diagram"></div>

      <h2 id="self-conditioning">Self-conditioning</h2>
      <p>
        So far the applications we have considered condition on <em>external</em>
        information, but it is also possible (and sometimes desireable) to
        condition on <em>internal</em> information &mdash; that is, to
        <em>self-condition</em> a network.
      </p>
      <p>
        In the speech recognition domain, <dt-cite key="kim2017dynamic"></dt-cite>
        modulates a deep bidirectional LSTM using a form of conditional
        normalization which the authors call <em>dynamic layer normalization</em>.
        In analogy to conditional instance normalization and conditional batch
        normalization, dynamic layer normalization can be seen as an instance of
        FiLM where the post-normalization feature-wise affine transformation is
        disabled and a FiLM layer is placed immediately after the instance
        normalization layer.
      </p>
      <div class="l-body-outset figure" id="dln-diagram"></div>
      <p>
        The key difference here is that the conditioning signal does not come
        from an external source, but is rather computed from utterance
        summarization feature vectors extracted in each layer of the network
        as a way of adapting the neural acoustic model.
      </p>
      <p>
        In the visual domain, the ImageNet 2017 winning model
        <dt-cite key="hu2017squeeze"></dt-cite> employs a self-conditioning
        scheme in the form of feature-wise sigmoidal gating as a way to
        condition a layer's activations on its previous layer.
      </p>
      <div class="l-body-outset figure" id="squeeze-diagram"></div>
      <p>
        In the natural language domain, <dt-cite key="gehring2017convolutional"></dt-cite>
        introduces a fast and parallelizable model for machine translation in
        the form of a purely convolutional architecture. Despite not being the
        main technical innovation, this model also uses feature-wise sigmoidal
        gating to condition a layer's activations on its previous
        layer.
      </p>
      <div class="l-body-outset figure" id="convseq2seq-diagram"></div>
      <p>
        From the FiLM point of view, feature-wise sigmoidal gating is
        equivalent to a FiLM layer with the scaling being passed through a
        sigmoidal non-linearity and the shifting set to 0.
      </p>
      <p>
        The LSTM <dt-cite key="hochreiter1997long"></dt-cite> architecture
        itself can be viewed from the perspective of self-conditioned
        feature-wise affine transformations: the input, forget, and output gates
        can be seen as FiLM layers with a sigmoid-constrained
        <dt-math>\gamma</dt-math> and with <dt-math>\beta = 0</dt-math>.
      </p>
      <div class="l-body-outset figure" id="lstm-diagram"></div>

      <h2 id="attention-over-features">Attention over features</h2>
      <p>
        Work in <dt-cite key="dhingra2017gated"></dt-cite> introduces the
        Gated-Attention Reader, which conditions a document reading network
        with an associated query via multiple steps of feature-wise sigmoidal
        gating, and work in <dt-cite key="chaplot2017gated"></dt-cite> trains a
        reinforcement learning (RL) agent to follow simple natural language
        instructions in a 3D environment (VizDoom) using the same feature-wise
        sigmoidal gating mechanism. Both papers show improvement over
        conditioning via concatenation.
      </p>
      <div class="l-body-outset figure" id="gated-attention-diagram"></div>
      <p>
        Feature-wise sigmoidal gating can be seen as inserting a FiLM layer
        with scaling constrained to be between 0 and 1 via a sigmoidal layer
        and shifting set to zero.
      </p>
      <p>
        Work in <dt-cite key="shen2017disan"></dt-cite> proposes a feature-wise
        attention mechanism for language understanding, showing strong results
        on datasets such as SNLI <dt-cite key="bowman2015large"></dt-cite>:
      </p>
      <p>
        <em>Word embedding usually suffers from the polysemy in natural language.
        Since traditional attention computes a single importance score for
        each word based on the word embedding, it cannot distinguish the
        meanings of the same word in different contexts. Multi-dimensional
        attention, however, computes a score for each feature of each word, so
        it can select the features that can best describe the word’s specific
        meaning in any given context, and include this information in the
        sentence encoding outputs. </em>
        <span class="todo"> TODO: this is verbatim, we need to change it.</span>
      </p>
      <p></p>
      <div class="l-body-outset figure" id="disan-diagram"></div>

      <h2 id="gating">Gating</h2>
      <p>
        Work on mixture-of-experts models
        <dt-cite key="jacobs1991adaptive,jordan1994hierarchical,eigen2014deep"></dt-cite>
        also exhibit a strong relationship with conditional feature-wise affine
        transformations.
      </p>
      <p><span class="todo"> TODO: explain the relationship.</span></p>
      <div class="l-body-outset figure" id="moe-diagram"></div>

      <hr />

      <h1>Understanding FiLM-ed networks</h1>


      <h2 id="film-as-task-embeddings">FiLM parameters as task embeddings</h2>
      <div class="l-body-outset figure" id="clevr-tsne"></div>
    </dt-article>

    <dt-appendix>
      <h3>Acknowledgements</h3>
      <p><span class="todo"> TODO: WRITEME.</span><p>

    <script type="text/bibliography">
      @inproceedings{vries2017modulating,
        author={de Vries, Harm and Strub, Florian and Mary, J\'{e}r\'{e}mie and
                Larochelle, Hugo and Pietquin, Olivier and Courville, Aaron},
        title={Modulating early visual processing by language},
        booktitle={Advances in Neural Information Processing Systems},
        year={2017},
        url={https://arxiv.org/pdf/1707.00683.pdf},
      }
      @inproceedings{perez2017learning,
        author={Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin,
                Vincent and Courville, Aaron},
        title={Learning visual reasoning without strong priors},
        booktitle={ICML Workshop on Machine Learning in Speech and Language Processing},
        year={2017},
        url={https://arxiv.org/pdf/1707.03017.pdf},
      }
      @inproceedings{perez2017film,
        author={Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin,
                Vincent and Courville, Aaron},
        title={FiLM: Visual Reasoning with a General Conditioning Layer},
        booktitle={arXiv},
        year={2017},
        url={https://arxiv.org/pdf/1709.07871.pdf},
      }
      @inproceedings{johnson2017clevr,
        author={Johnson, Justin and Li, Fei-Fei and Hariharan, Bharath and Zitnick,
                Lawrence C. and van der Maaten, Laurens and Girshick, Ross},
        title={FiLM: Visual Reasoning with a General Conditioning Layer},
        booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition},
        year={2017},
        url={https://arxiv.org/pdf/1612.06890.pdf},
      }
      @inproceedings{dumoulin2017learned,
        author={Dumoulin, Vincent and Shlens, Jonathon and Kudlur, Manjunath},
        title={A Learned Representation for Artistic Style},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2017},
        url={https://arxiv.org/pdf/1610.07629.pdf},
      }
      @inproceedings{ghiasi2017exploring,
        author={Ghiasi, Golnaz and Lee, Honglak and Kudlur, Manjunath and
                Dumoulin, Vincent and Shlens, Jonathon},
        title={Exploring the structure of a real-time, arbitrary neural artistic
               stylization network},
        booktitle={Proceedings of the British Machine Vision Conference},
        year={2017},
        url={https://arxiv.org/pdf/1705.06830.pdf},
      }
      @inproceedings{huang2017arbitrary,
        title={Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},
        author={Huang, Xun and Belongie, Serge},
        booktitle={Proceedings of the International Conference on Computer Vision},
        year={2017},
        url={https://arxiv.org/pdf/1703.06868.pdf},
      }
      @inproceedings{radford2016unsupervised,
        author={Radford, Alec and Metz, Luke and Chintala, Soumith},
        title={Unsupervised Representation Learning with Deep Convolutional
               Generative Adversarial Networks},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2016},
        url={https://arxiv.org/pdf/1511.06434.pdf},
      }
      @inproceedings{oord2016conditional,
        title={Conditional Image Generation with PixelCNN Decoders},
        author={van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse
                and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
        booktitle={Advances in Neural Information Processing Systems},
        year={2016},
        url={https://arxiv.org/pdf/1606.05328.pdf}
      }
      @article{oord2016wavenet,
        title={WaveNet: A Generative Model for Raw Audio},
        author={van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and
                Simonyan, Karen and Vinyals, Oriol and Graves, Alex and
                Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
        journal={arXiv},
        year={2016},
        url={https://arxiv.org/pdf/1609.03499.pdf},
      }
      @inproceedings{ha2016hypernetworks,
        author={Ha, David and Dai, Andrew and Le, Quoc},
        title={HyperNetworks},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2016},
        url={https://arxiv.org/pdf/1609.09106.pdf},
      }
      @inproceedings{kim2017dynamic,
        author={Kim, Taesup and Song, Inchul and Bengio, Yoshua},
        title={Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling
               in Speech Recognition},
        booktitle={Interspeech},
        year={2017},
        url={https://arxiv.org/pdf/1707.06065.pdf},
      }
      @inproceedings{hu2017squeeze,
        author={Hu, Jie and Shen, Li and Sun, Gang},
        title={Squeeze-and-Excitation Networks},
        booktitle={CVPR's ILSVRC 2017 Workshop},
        year={2017},
        url={https://arxiv.org/pdf/1709.01507.pdf},
      }
      @inproceedings{gehring2017convolutional,
        author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats,
                Denis and Dauphin, Yann N.},
        title={Convolution Sequence-to-Sequence Learning},
        booktitle={Proceedings of the International Conference on Machine Learning},
        year={2017},
        url={https://arxiv.org/pdf/1705.03122.pdf},
      }
      @article{hochreiter1997long,
        title={Long Short-Term Memory},
        author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
        journal={Neural Computation},
        volume={9},
        number={8},
        pages={1735--1780},
        year={1997},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1997.9.8.1735},
      }
      @article{tenenbaum2000separating,
        title={Separating Style and Content with Bilinear Models},
        author={Tenenbaum, Joshua B. and Freeman, William T.},
        journal={Neural Computation},
        volume={12},
        number={6},
        pages={1247--1283},
        year={2000},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/089976600300015349},
      }
      @article{dhingra2017gated,
        title={Gated-Attention Readers for Text Comprehension},
        author={Dhingra, Bhuwan and Liu, Hanxiao and Yang, Zhilin, and
                Cohen, William W and Salakhutdinov, Ruslan},
        booktitle={Proceedings of the Annual Meeting of the Association for
                   Computational Linguistics},
        year={2017},
        url={https://arxiv.org/pdf/1606.01549.pdf},
      }
      @inproceedings{chaplot2017gated,
        title={Gated-Attention Architectures for Task-Oriented Language Grounding},
        author={Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and
                Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov,
                Ruslan},
        booktitle={ACL Workshop on Language Grounding for Robotics},
        year={2017},
        url={https://arxiv.org/pdf/1706.07230.pdf},
      }
      @inproceedings{shen2017disan,
        title={DiSAN: Directional Self-Attention Network for RNN/CNN-free
               Language Understanding},
        author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and
                Pan, Shirui and Zhang, Chengqi},
        booktitle={arXiv},
        year={2017},
        url={https://arxiv.org/pdf/1709.04696.pdf},
      }
      @inproceedings{bowman2015large,
        title={A large annotated corpus for learning natural language inference},
        author={Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and
                Manning, Christopher D.},
        booktitle={Proceedings of the Conference on Empirical Methods in Natural
                   Language Processing},
        year={2015},
        url={https://nlp.stanford.edu/pubs/snli_paper.pdf},
      }
      @article{jacobs1991adaptive,
        title={Adaptive Mixtures of Local Experts},
        author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J.
                and Hinton, Geoffrey E.},
        journal={Neural Computation},
        volume={3},
        number={1},
        pages={79--87},
        year={1991},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1991.3.1.79},
      }
      @article{jordan1994hierarchical,
        title={Hierarchical Mixtures of Experts and the EM Algorithm},
        author={Jordan, Michael I. and Jacobs, Robert A.},
        journal={Neural Computation},
        volume={6},
        number={2},
        pages={181--214},
        year={1994},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1994.6.2.181},
      }
      @inproceedings{eigen2014deep,
        author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
        title={Learning Factored Representations in a Deep Mixture of Experts},
        booktitle={Proceedings of the ICLR Workshops},
        year={2014},
        url={https://arxiv.org/pdf/1312.4314.pdf},
      }
      @article{kirkpatrick2017overcoming,
        title={Overcoming catastrophic forgetting in neural networks},
        author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and
                Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and
                Milan, Kieran and Quan, John and Ramalho, Tiago and
                Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath,
                Claudia and Kumaran, Dharshan and Hadsell, Raia},
        journal={Proceedings of the National Academy of Sciences},
        volume={114},
        number={13},
        pages={3521--3526},
        year={2017},
        url={http://www.pnas.org/content/114/13/3521.abstract},
      }
      @inproceedings{mel1992clusteron,
        title={The Clusteron: Toward a Simple Abstraction for a Complex Neuron},
        author={Mel, Bartlett W},
        booktitle={Advances in Neural Information Processing Systems},
        year={1992},
        url={http://papers.nips.cc/paper/450-the-clusteron-toward-a-simple-abstraction-for-a-complex-neuron.pdf},
      }
      @article{boutonnet2015words,
        title={Words jump-start vision: A label advantage in object recognition},
        author={B. Boutonnet and G. Lupyan},
        journal={Journal of Neuroscience},
        volume={35},
        number={25},
        pages={9329--9335},
        year={2015},
        publisher={Society for Neuroscience},
        url={},
      }
    </script>
    <script src="assets/figure_film.js"></script>
    <script src="assets/figure_film_architecture.js"></script>
    <script src="assets/figure_bilinear.js"></script>
    <script src="assets/figure_clevr.js"></script>
    <script src="assets/figure_guesswhat.js"></script>
    <script src="assets/figure_alrfas.js"></script>
    <script src="assets/figure_adain.js"></script>
    <script src="assets/figure_dcgan.js"></script>
    <script src="assets/figure_pixelcnn.js"></script>
    <script src="assets/figure_wavenet.js"></script>
    <script src="assets/figure_dln.js"></script>
    <script src="assets/figure_squeeze.js"></script>
    <script src="assets/figure_convseq2seq.js"></script>
    <script src="assets/figure_lstm.js"></script>
    <script src="assets/figure_gated_attention.js"></script>
    <script src="assets/figure_disan.js"></script>
    <script src="assets/figure_moe.js"></script>
    <script src="assets/figure_rl.js"></script>
    <script src="assets/figure_clevr_tsne.js"></script>
  </body>
</html>
